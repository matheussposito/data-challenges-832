{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers* ? \n",
    "    - how many *parameters* are involved in such a layer ?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started !</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits ? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, *Yann Le Cun* used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Nota Bene: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, the CNN predicts what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**] üìö(http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 21:16:26.412674: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-02 21:16:26.412731: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just some arrangements done Matplotlib...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4cc061b760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4cc051d100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[16], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: As a first preprocessing step, please normalize your data.** ‚ùì\n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*NB: you can also center your data, by substracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train_norm = X_train / 255\n",
    "X_test_norm = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: What is the shape of your images** ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ You see that you have 60.000 training images, all of size $(28, 28)$. However...\n",
    "\n",
    "‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`** ‚ùóÔ∏è\n",
    "\n",
    "This last dimension is clearly missing here... Can you guess the reason why ?\n",
    "\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ 28 \\times 28 $ pictures are black-and-white. $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1)\n",
    "        \n",
    "* Theoretically, you don't need to know the number of channels unlike colored pictures using for example:\n",
    "    - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "    - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span> </b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **expand_dims** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test` which should respectively be equal to  $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 21:16:29.828864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-02 21:16:29.828949: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-02 21:16:29.828980: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-O26C6N05): /proc/driver/nvidia/version does not exist\n",
      "2022-03-02 21:16:29.829438: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X_trainex = expand_dims(X_train_norm)\n",
    "X_testex = expand_dims(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([60000, 28, 28, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last thing to do to prepare your data is to convert your labels to \"*one-hot encode*\" the categories.\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $ (2, 2) $\n",
    "- a second `Conv2D` layer with 16 filters, each of size $ (3, 3) $, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $ (2, 2) $\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` \n",
    "* with the `adam` optimizer\n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*NB: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), activation='relu', padding='same', input_shape=(28,28,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convoluational layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,264\n",
      "Trainable params: 9,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 10 epochs max in this challenge, just to save some precious time for the more advanced challenges... !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 [==============================] - 18s 13ms/step - loss: 0.4140 - accuracy: 0.8687 - val_loss: 0.1574 - val_accuracy: 0.9523\n",
      "Epoch 2/10\n",
      "1313/1313 [==============================] - 17s 13ms/step - loss: 0.1199 - accuracy: 0.9635 - val_loss: 0.1177 - val_accuracy: 0.9635\n",
      "Epoch 3/10\n",
      "1313/1313 [==============================] - 17s 13ms/step - loss: 0.0876 - accuracy: 0.9727 - val_loss: 0.0898 - val_accuracy: 0.9724\n",
      "Epoch 4/10\n",
      "1313/1313 [==============================] - 16s 12ms/step - loss: 0.0719 - accuracy: 0.9781 - val_loss: 0.0879 - val_accuracy: 0.9730\n",
      "Epoch 5/10\n",
      "1313/1313 [==============================] - 17s 13ms/step - loss: 0.0616 - accuracy: 0.9813 - val_loss: 0.0781 - val_accuracy: 0.9767\n",
      "Epoch 6/10\n",
      "1313/1313 [==============================] - 17s 13ms/step - loss: 0.0543 - accuracy: 0.9831 - val_loss: 0.0642 - val_accuracy: 0.9807\n",
      "Epoch 7/10\n",
      "1313/1313 [==============================] - 18s 13ms/step - loss: 0.0496 - accuracy: 0.9843 - val_loss: 0.0633 - val_accuracy: 0.9809\n",
      "Epoch 8/10\n",
      "1313/1313 [==============================] - 18s 13ms/step - loss: 0.0443 - accuracy: 0.9862 - val_loss: 0.0641 - val_accuracy: 0.9812\n",
      "Epoch 9/10\n",
      "1313/1313 [==============================] - 17s 13ms/step - loss: 0.0413 - accuracy: 0.9868 - val_loss: 0.0610 - val_accuracy: 0.9821\n",
      "Epoch 10/10\n",
      "1313/1313 [==============================] - 17s 13ms/step - loss: 0.0359 - accuracy: 0.9881 - val_loss: 0.0637 - val_accuracy: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c4131ed30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_trainex, y_train_cat, batch_size=32, epochs=10, validation_split=0.3, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer:</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ images from the *train images*.\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $  batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0549 - accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "performance = model.evaluate(X_testex, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05485627427697182, 0.984499990940094]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
